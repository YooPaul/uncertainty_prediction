# -*- coding: utf-8 -*-
"""CSE571_Proj1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Md9h0sAu0WL-mLSre1AZiTGDkDHgHCC
"""

from google.colab import files
files.upload()

import torch
import numpy as np
import torch.nn as nn
from torch.autograd import Variable
from sklearn.model_selection import train_test_split

dataset = np.load('my_training_data_noGP_50ep.npz')
full_dataset_x = dataset['x']
full_dataset_y = dataset['y']

train_dataset_x, val_dataset_x, train_dataset_y, val_dataset_y = train_test_split(full_dataset_x, full_dataset_y, test_size = 0.4)


# training_x = torch.tensor(full_dataset_x)
# training_y = torch.tensor(full_dataset_y)

# train_size_x = int(0.6 * len(full_dataset_x))
# val_size_x = len(full_dataset_x) - train_size_x

# train_dataset_x, val_dataset_x = torch.utils.data.random_split(full_dataset_x, [train_size_x, val_size_x])
# train_dataset_y, val_dataset_y = torch.utils.data.random_split(full_dataset_y, [train_size_x, val_size_x])

training_x = torch.tensor(train_dataset_x)
val_x = torch.tensor(val_dataset_x)
training_y = torch.tensor(train_dataset_y)
val_y = torch.tensor(val_dataset_y)

INPUT_SIZE, N_HIDDEN, OUTPUT_SIZE = 6, 50, 4

model = nn.Sequential(
    nn.Linear(INPUT_SIZE, N_HIDDEN),
    nn.ReLU(),
    nn.Dropout(0.6),
    nn.Linear(N_HIDDEN, N_HIDDEN),
    nn.ReLU(),
    nn.Dropout(0.6),
    nn.Linear(N_HIDDEN, OUTPUT_SIZE),
    #nn.Sigmoid()
)


criterion = nn.MSELoss()  # Specify loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Specify which optimizer

y_hats = []
epochs = 5000
losses = []
# Model Training
for i in range(epochs):
    y_pred = model(training_x.float())
    loss = criterion(y_pred, training_y.float())
    if i % 500 == 0:
      # print(y_pred)
      # print(training_y)
      print("epoch:", i, "loss", loss.item())
    losses.append(loss.item())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if i % 500 == 0:
        model.eval()
        y_pred = model(val_x.float())
        loss = criterion(y_pred, val_y.float())
        print("validate loss", loss.item())
        model.train()

    if i in [0, epochs / 2, epochs - 1]:
      X = Variable(torch.from_numpy(full_dataset_x).type(torch.FloatTensor))
      Y_hat = np.array([model(X).data.cpu().numpy() for _ in range(1000)]).squeeze()

      Y_mean = Y_hat.mean(axis=0)
      Y_std = Y_hat.std(axis=0)
      y_hats.append((Y_mean, Y_std))

  # Test Model
  # model.eval()
  # y_pred = model(val_x.float())

# Predicting uncertainty
'''
X = Variable(torch.from_numpy(full_dataset_x).type(torch.FloatTensor))
model = model.train()
Y_hat = np.array([model(X).data.cpu().numpy() for _ in range(1000)]).squeeze()

Y_mean = Y_hat.mean(axis=0)
Y_std = Y_hat.std(axis=0)

print("Mean:", Y_mean)
print("Std:", Y_std)'''

import matplotlib.pyplot as plt
N = 64
data_point = 2
x = full_dataset_y[data_point * N:data_point * N + N, 0]
y = Y_mean[data_point * N:data_point * N + N, 0]
y_err = Y_std[data_point * N:data_point * N + N,0]
plt.plot(x,y,'bo')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

data_point += 1
x = full_dataset_y[data_point * N:data_point * N + N, 0]
y = Y_mean[data_point * N:data_point * N + N, 0]
y_err = Y_std[data_point * N:data_point * N + N,0]
plt.plot(x,y,'go')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')


data_point += 1
x = full_dataset_y[data_point * N:data_point * N + N, 0]
y = Y_mean[data_point * N:data_point * N + N, 0]
y_err = Y_std[data_point * N:data_point * N + N,0]
plt.plot(x,y,'yo')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')


plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()

import matplotlib.pyplot as plt
N = 64
data_point = 2
ouput = 3
x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[0][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[0][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'bo', label = 'Initial training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[1][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[1][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'go', label= 'Middle training epoch ')
plt.errorbar(x,y,yerr=y_err, fmt = 'g ')


x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[2][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[2][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'yo', label='Final training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

plt.legend()
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show();

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
N = 64
data_point = 3
ouput = 3
x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[0][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[0][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'bo', label = 'Initial training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[1][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[1][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'go', label= 'Middle training epoch ')
plt.errorbar(x,y,yerr=y_err, fmt = 'g ')


x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[2][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[2][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'yo', label='Final training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

plt.legend()
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show();

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
N = 64
data_point = 4
ouput = 3
x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[0][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[0][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'bo', label = 'Initial training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[1][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[1][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'go', label= 'Middle training epoch ')
plt.errorbar(x,y,yerr=y_err, fmt = 'g ')


x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[2][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[2][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'yo', label='Final training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

plt.legend()
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show();



# Ensemble Method

import torch
import numpy as np
import torch.nn as nn
from torch.autograd import Variable
from sklearn.model_selection import train_test_split

dataset = np.load('my_training_data_noGP_50ep.npz')
full_dataset_x = dataset['x']
full_dataset_y = dataset['y']

train_dataset_x, val_dataset_x, train_dataset_y, val_dataset_y = train_test_split(full_dataset_x, full_dataset_y, test_size = 0.4)

training_x = torch.tensor(train_dataset_x)
val_x = torch.tensor(val_dataset_x)
training_y = torch.tensor(train_dataset_y)
val_y = torch.tensor(val_dataset_y)

INPUT_SIZE, N_HIDDEN, OUTPUT_SIZE = 6, 50, 4

N = 10
models = [nn.Sequential(
    nn.Linear(INPUT_SIZE, N_HIDDEN),
    nn.ReLU(),
    nn.Linear(N_HIDDEN, N_HIDDEN),
    nn.ReLU(),
    nn.Linear(N_HIDDEN, OUTPUT_SIZE),
    #nn.Sigmoid()
  ) for _ in range(N)]
models = [(model, nn.MSELoss(), torch.optim.Adam(model.parameters(), lr=0.001)) for model in models]


#criterion = nn.MSELoss()  # Specify loss function
#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Specify which optimizer

y_hats = []
epochs = 3000
losses = []
# Model Training
for i in range(epochs):
  for model, criterion, optimizer in models:
    y_pred = model(training_x.float())
    loss = criterion(y_pred, training_y.float())
    if i % 1000 == 0:
      # print(y_pred)
      # print(training_y)
      print("epoch:", i, "loss", loss.item())
    losses.append(loss.item())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if i % 1000 == 0:
        model.eval()
        y_pred = model(val_x.float())
        loss = criterion(y_pred, val_y.float())
        print("validate loss", loss.item())
        model.train()

    if i in [0, epochs / 2, epochs - 1]:
      X = Variable(torch.from_numpy(full_dataset_x).type(torch.FloatTensor))
      Y_hat = []
      for model,_,_ in models:
        model = model.eval()
        Y_hat.append(model(X).data.cpu().numpy())
      Y_hat = np.array(Y_hat).squeeze()

      Y_mean = Y_hat.mean(axis=0)
      Y_std = Y_hat.std(axis=0)
      y_hats.append((Y_mean, Y_std))

  # Test Model
  # model.eval()
  # y_pred = model(val_x.float())

# Predicting uncertainty
'''
X = Variable(torch.from_numpy(full_dataset_x).type(torch.FloatTensor))
Y_hat = []
for model,_,_ in models:
  model = model.eval()
  Y_hat.append(model(X).data.cpu().numpy())
Y_hat = np.array(Y_hat).squeeze()

Y_mean = Y_hat.mean(axis=0)
Y_std = Y_hat.std(axis=0)

print("Mean:", Y_mean)
print("Std:", Y_std)''';

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
N = 64
data_point = 4
ouput = 3
x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[0][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[0][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'bo', label = 'Initial training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[1][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[1][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'go', label= 'Middle training epoch ')
plt.errorbar(x,y,yerr=y_err, fmt = 'g ')


x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[2][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[2][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'yo', label='Final training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

plt.legend()
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show();

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
N = 64
data_point = 4
ouput = 3
x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[0][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[0][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'bo', label = 'Initial training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[1][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[1][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'go', label= 'Middle training epoch ')
plt.errorbar(x,y,yerr=y_err, fmt = 'g ')


x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[2][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[2][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'yo', label='Final training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

plt.legend()
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show();

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
N = 64
data_point = 4
ouput = 3
x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[0][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[0][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'bo', label = 'Initial training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[1][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[1][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'go', label= 'Middle training epoch ')
plt.errorbar(x,y,yerr=y_err, fmt = 'g ')


x = full_dataset_y[data_point * N:data_point * N + N, ouput]
y = y_hats[2][0][data_point * N:data_point * N + N, ouput]
y_err = y_hats[2][1][data_point * N:data_point * N + N,ouput]
plt.plot(x,y,'yo', label='Final training epoch')
plt.errorbar(x,y,yerr=y_err, fmt = 'r ')

plt.legend()
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show();

